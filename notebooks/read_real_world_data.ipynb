{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5741067-39ef-4311-a4ca-6a3a1ac86f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcbba6e-0b51-4783-8764-fd5bc19dfb2b",
   "metadata": {},
   "source": [
    "# load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4300fed5-f869-4240-a3db-5091b2d11171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydantic model\n",
    "\n",
    "class PreprocessingSteps(BaseModel):\n",
    "    transformers: Tuple[\n",
    "        Tuple[PCA, StandardScaler, np.ndarray, np.ndarray],\n",
    "        Tuple[float, float]\n",
    "    ] = Field(\n",
    "        ..., \n",
    "        description=\"Preprocessing steps including PCA, StandardScaler, and arrays\"\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "        json_encoders = {\n",
    "            np.ndarray: lambda v: v.tolist(),\n",
    "            PCA: lambda v: str(v),\n",
    "            StandardScaler: lambda v: str(v)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3299088-7b44-4250-9db7-00d2317df289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset: str, activation: str = \"relu\") -> dict:\n",
    "    \"\"\"\n",
    "    Load data from pickle files for a specific dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset (str): Name of the dataset or specific subdirectory\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with full file paths as keys and loaded data as values\n",
    "    \"\"\"\n",
    "    \n",
    "    base_dir = \"../results/datasets/real_world_data/\"\n",
    "    if dataset in [\"circles\", \"spheres\", \"alternate_stripes\"]:\n",
    "        base_dir = \"../results/datasets/synthetic_data\"\n",
    "    \n",
    "    full_path = os.path.join(base_dir, dataset)\n",
    "    full_path = full_path + f\"/activation={activation}\"\n",
    "\n",
    "    print(full_path)\n",
    "    \n",
    "    data_dict = {}\n",
    "    \n",
    "    # Walk through the directory tree starting from the specified dataset path\n",
    "    for root, dirs, files in os.walk(full_path):\n",
    "        for file in files:\n",
    "            # Check if the file is a pickle file\n",
    "            if file.endswith('.pkl') or file.endswith('.p'):\n",
    "                full_file_path = os.path.join(root, file)\n",
    "                \n",
    "                try:\n",
    "                    # Load the pickle file\n",
    "                    with open(full_file_path, \"rb\") as f:\n",
    "                        data = pickle.load(f)\n",
    "                    \n",
    "                    # Store in the dictionary with full path as key\n",
    "                    data_dict[full_file_path] = data\n",
    "                \n",
    "                except (IOError, pickle.UnpicklingError) as e:\n",
    "                    print(f\"Error loading {full_file_path}: {e}\")\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "362faac8-fe45-41ed-9d17-bc644f543476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_nested_files(dataset: str, activation: str = \"relu\"):\n",
    "\n",
    "    base_dir = \"../results/datasets/real_world_data/\"\n",
    "    if dataset in [\"circles\", \"spheres\", \"alternate_stripes\"]:\n",
    "        base_dir = \"../results/datasets/synthetic_data\"\n",
    "    \n",
    "    full_path = os.path.join(base_dir, dataset)\n",
    "    full_path = full_path + f\"/activation={activation}\"\n",
    "    \n",
    "    # Dictionary to store file contents with file paths as keys\n",
    "    file_contents = {}\n",
    "    \n",
    "    # Walk through all directories and files\n",
    "    for root, dirs, files in os.walk(full_path):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            if \"results_list.p\" in file_path:\n",
    "                print(file_path)\n",
    "                try:\n",
    "                    content = pickle.load(open(file_path, 'rb'))\n",
    "                    # Store content with full path as key\n",
    "                    file_contents[file_path] = content\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not read {file_path}: {e}\")\n",
    "    \n",
    "    return file_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd83ee27-52c8-4445-b7ce-e64451e4f3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b57422d-a837-4228-a4b2-6c71bb96cf18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d775b7de-f7e9-48be-8171-367dff4f2e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6ca4b75-dad1-4de5-abab-cf3d58b8fb79",
   "metadata": {},
   "source": [
    "# functions to collate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe7736cf-c9b2-4227-b971-c50d85bec2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the explained variance objectives for train and validation\n",
    "\n",
    "def extract_explained_variance(data: list[list[PreprocessingSteps]]) -> np.array:\n",
    "\n",
    "    explained_variance = []\n",
    "    \n",
    "    for j in range(len(data)):\n",
    "        \n",
    "        data_run = [data[j][i][-1] for i in range(len(data[j]))]\n",
    "        print(\"WE ARE HERE\", type(data_run), len(data_run))\n",
    "        explained_variance.append(data_run)\n",
    "\n",
    "    return np.array(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e016e4b-c70f-432b-8425-091f5cc85e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94fa952c-8ff2-4746-a443-7412f41eff94",
   "metadata": {},
   "source": [
    "# visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "704bba60-93e4-4064-8240-436ae65bedca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_percentiles(partial_data, full_data, dataset: str):\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Colors for different contribution types\n",
    "    colors = {'partial': 'darkred', 'full': 'darkblue'}\n",
    "    labels = ['training', 'validation']\n",
    "    linestyles = ['--', '-']\n",
    "\n",
    "    activation_fn = 'relu'\n",
    "    if dataset == 'alternate_stripes':\n",
    "        activation_fn = 'cos'\n",
    "\n",
    "    legend_list = [f\"h={activation_fn}, objective=partial\",\n",
    "                   f\"h={activation_fn}, objective=partial\",\n",
    "                   f\"h={activation_fn}, objective=full\",\n",
    "                   f\"h={activation_fn}, objective=full\"]\n",
    "    legend_entries = []\n",
    "    \n",
    "    # Plot both partial and full contributions\n",
    "    enumerate_counter = 0\n",
    "    for data, contrib_type in [(partial_data, 'partial'), (full_data, 'full')]:\n",
    "        # Compute percentiles along the first axis (runs)\n",
    "        p20 = np.percentile(data, 20, axis=0)\n",
    "        p50 = np.percentile(data, 50, axis=0)\n",
    "        p80 = np.percentile(data, 80, axis=0)\n",
    "        \n",
    "        # Create x-axis (time steps)\n",
    "        x = np.arange(data.shape[1])\n",
    "        \n",
    "        # Plot for each dimension (training/validation)\n",
    "        for dim in range(2):\n",
    "            plt.plot(x, p50[:, dim], color=colors[contrib_type], \n",
    "                    label=f'{contrib_type.capitalize()} {labels[dim]} Median', \n",
    "                    linestyle=linestyles[dim], linewidth=1.0)\n",
    "            if dim == 0:\n",
    "                legend_entries.append(legend_list[enumerate_counter])\n",
    "            else:\n",
    "                legend_entries.append(f\"_\")\n",
    "\n",
    "            plt.fill_between(x, p20[:, dim], p80[:, dim], \n",
    "                           alpha=0.2, color=colors[contrib_type])\n",
    "            legend_entries.append(f\"_\")\n",
    "\n",
    "            enumerate_counter += 1\n",
    "    \n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend(loc='upper left', labels=legend_entries)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # path_to_save = f\"../results/plots/training_curve/{dataset}.pdf\"\n",
    "    # directory = os.path.dirname(path_to_save)\n",
    "    # if not os.path.exists(directory):\n",
    "    #     os.makedirs(directory)\n",
    "        \n",
    "    # plt.savefig(path_to_save, dpi=300)\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa6324d3-4b46-4304-91ce-3015da65f457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=True/9/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=True/0/results_list.p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomas/Library/Caches/pypoetry/virtualenvs/es-pca-6NHe3mCZ-py3.13/lib/python3.13/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator PCA from version 1.4.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/thomas/Library/Caches/pypoetry/virtualenvs/es-pca-6NHe3mCZ-py3.13/lib/python3.13/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.4.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=True/11/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=True/7/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=True/6/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=True/1/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=True/10/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=True/8/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=True/4/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=True/3/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=True/12/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=True/2/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=True/13/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=True/5/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=True/14/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=False/9/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=False/0/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=False/11/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=False/7/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=False/6/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=False/1/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=False/10/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=False/8/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=False/4/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=False/3/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=False/12/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=False/2/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=False/13/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=False/5/results_list.p\n",
      "../results/datasets/real_world_data/german_credit/activation=relu/partial_contrib=False/14/results_list.p\n",
      "WE ARE HERE <class 'list'> 15001\n",
      "WE ARE HERE <class 'list'> 15001\n",
      "WE ARE HERE <class 'list'> 15001\n",
      "WE ARE HERE <class 'list'> 15001\n",
      "WE ARE HERE <class 'list'> 15001\n",
      "WE ARE HERE <class 'list'> 15001\n",
      "WE ARE HERE <class 'list'> 15001\n",
      "WE ARE HERE <class 'list'> 15001\n",
      "WE ARE HERE <class 'list'> 15001\n",
      "WE ARE HERE <class 'list'> 15001\n",
      "WE ARE HERE <class 'list'> 15001\n",
      "WE ARE HERE <class 'list'> 15001\n",
      "WE ARE HERE <class 'list'> 15001\n",
      "WE ARE HERE <class 'list'> 15001\n",
      "WE ARE HERE <class 'list'> 15001\n",
      "WE ARE HERE <class 'list'> 2801\n",
      "WE ARE HERE <class 'list'> 2801\n",
      "WE ARE HERE <class 'list'> 2601\n",
      "WE ARE HERE <class 'list'> 2801\n",
      "WE ARE HERE <class 'list'> 2601\n",
      "WE ARE HERE <class 'list'> 2601\n",
      "WE ARE HERE <class 'list'> 2601\n",
      "WE ARE HERE <class 'list'> 2601\n",
      "WE ARE HERE <class 'list'> 2601\n",
      "WE ARE HERE <class 'list'> 2601\n",
      "WE ARE HERE <class 'list'> 2601\n",
      "WE ARE HERE <class 'list'> 2601\n",
      "WE ARE HERE <class 'list'> 2601\n",
      "WE ARE HERE <class 'list'> 2801\n",
      "WE ARE HERE <class 'list'> 2601\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (15,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     12\u001b[39m full_contrib_data = [value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m data_dictionary.items() \n\u001b[32m     13\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mpartial_contrib=False\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m key \u001b[38;5;129;01mand\u001b[39;00m activation \u001b[38;5;129;01min\u001b[39;00m key \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mresults_list.p\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m key]\n\u001b[32m     15\u001b[39m explained_variance_partial = extract_explained_variance(partial_contrib_data)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m explained_variance_full = \u001b[43mextract_explained_variance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_contrib_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Plot both contributions on the same figure\u001b[39;00m\n\u001b[32m     19\u001b[39m plot_percentiles(explained_variance_partial, explained_variance_full, dataset)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mextract_explained_variance\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWE ARE HERE\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(data_run), \u001b[38;5;28mlen\u001b[39m(data_run))\n\u001b[32m     11\u001b[39m     explained_variance.append(data_run)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexplained_variance\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (15,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Modified main loop\n",
    "# dataset_list = [\"alternate_stripes\", \"circles\", \"spheres\", \"wine\", \"heart-statlog\", \"ionosphere\", \"breast-cancer\"]\n",
    "dataset_list = [\"german_credit\"]\n",
    "for dataset in dataset_list:\n",
    "    activation = \"relu\"\n",
    "    data_dictionary = read_all_nested_files(dataset)\n",
    "    if dataset == \"alternate_stripes\":\n",
    "        activation = \"cos\"\n",
    "    \n",
    "    partial_contrib_data = [value for key, value in data_dictionary.items() \n",
    "                          if 'partial_contrib=True' in key and activation in key and \"results_list.p\" in key]\n",
    "    full_contrib_data = [value for key, value in data_dictionary.items() \n",
    "                        if 'partial_contrib=False' in key and activation in key and \"results_list.p\" in key]\n",
    "    \n",
    "    explained_variance_partial = extract_explained_variance(partial_contrib_data)\n",
    "    explained_variance_full = extract_explained_variance(full_contrib_data)\n",
    "    \n",
    "    # Plot both contributions on the same figure\n",
    "    plot_percentiles(explained_variance_partial, explained_variance_full, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ba4a2-0c4d-4a68-9e07-2a3e01c6ef5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06971d21-4e16-4c52-8ad4-b36566495052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2d1fc-8852-4254-9eee-ae942ad8fca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61809b86-07da-4cbf-8844-1808dc625f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1ce38-d683-43f1-bde6-62fa5ac13574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b83e109-feb4-4c6c-aadf-4ac970edd8e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1846c09b-f37a-408c-8dbc-97077704f3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50303d3-cc38-44f9-bb16-d61b3765ba2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ce457-2859-4871-8af0-f442c019da5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c51ee9-e366-4962-abbd-f38a3fb0f0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273943fe-06b5-42cc-96ad-cd029525fa27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
