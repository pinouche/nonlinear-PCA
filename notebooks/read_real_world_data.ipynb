{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5741067-39ef-4311-a4ca-6a3a1ac86f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcbba6e-0b51-4783-8764-fd5bc19dfb2b",
   "metadata": {},
   "source": [
    "# load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4300fed5-f869-4240-a3db-5091b2d11171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydantic model\n",
    "\n",
    "class PreprocessingSteps(BaseModel):\n",
    "    transformers: Tuple[\n",
    "        Tuple[PCA, StandardScaler, np.ndarray, np.ndarray],\n",
    "        Tuple[float, float]\n",
    "    ] = Field(\n",
    "        ..., \n",
    "        description=\"Preprocessing steps including PCA, StandardScaler, and arrays\"\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "        json_encoders = {\n",
    "            np.ndarray: lambda v: v.tolist(),\n",
    "            PCA: lambda v: str(v),\n",
    "            StandardScaler: lambda v: str(v)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3299088-7b44-4250-9db7-00d2317df289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset: str, activation: str = \"relu\") -> dict:\n",
    "    \"\"\"\n",
    "    Load data from pickle files for a specific dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset (str): Name of the dataset or specific subdirectory\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with full file paths as keys and loaded data as values\n",
    "    \"\"\"\n",
    "    \n",
    "    base_dir = \"../results/datasets/real_world_data\"\n",
    "    if dataset in [\"circles\", \"spheres\", \"alternate_stripes\"]:\n",
    "        base_dir = \"../results/datasets/synthetic_data\"\n",
    "    \n",
    "    full_path = os.path.join(base_dir, dataset)\n",
    "    \n",
    "    data_dict = {}\n",
    "    \n",
    "    # Walk through the directory tree starting from the specified dataset path\n",
    "    for root, dirs, files in os.walk(full_path):\n",
    "        for file in files:\n",
    "            # Check if the file is a pickle file\n",
    "            if file.endswith('.pkl') or file.endswith('.p'):\n",
    "                full_file_path = os.path.join(root, file)\n",
    "                \n",
    "                try:\n",
    "                    # Load the pickle file\n",
    "                    with open(full_file_path, \"rb\") as f:\n",
    "                        data = pickle.load(f)\n",
    "                    \n",
    "                    # Store in the dictionary with full path as key\n",
    "                    data_dict[full_file_path] = data\n",
    "                \n",
    "                except (IOError, pickle.UnpicklingError) as e:\n",
    "                    print(f\"Error loading {full_file_path}: {e}\")\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca4b75-dad1-4de5-abab-cf3d58b8fb79",
   "metadata": {},
   "source": [
    "# functions to collate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7736cf-c9b2-4227-b971-c50d85bec2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the explained variance objectives for train and validation\n",
    "\n",
    "def extract_explained_variance(data: list[list[PreprocessingSteps]]) -> np.array:\n",
    "\n",
    "    explained_variance = []\n",
    "    \n",
    "    for j in range(len(data)):\n",
    "        \n",
    "        data_run = [data[j][i][-1] for i in range(len(data[j]))]\n",
    "        explained_variance.append(data_run)\n",
    "  \n",
    "    return np.array(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e016e4b-c70f-432b-8425-091f5cc85e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94fa952c-8ff2-4746-a443-7412f41eff94",
   "metadata": {},
   "source": [
    "# visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "704bba60-93e4-4064-8240-436ae65bedca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_percentiles(partial_data, full_data, dataset: str):\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Colors for different contribution types\n",
    "    colors = {'partial': 'darkred', 'full': 'darkblue'}\n",
    "    labels = ['training', 'validation']\n",
    "    linestyles = ['--', '-']\n",
    "\n",
    "    activation_fn = 'relu'\n",
    "    if dataset == 'alternate_stripes':\n",
    "        activation_fn = 'cos'\n",
    "\n",
    "    legend_list = [f\"h={activation_fn}, objective=partial\",\n",
    "                   f\"h={activation_fn}, objective=partial\",\n",
    "                   f\"h={activation_fn}, objective=full\",\n",
    "                   f\"h={activation_fn}, objective=full\"]\n",
    "    legend_entries = []\n",
    "    \n",
    "    # Plot both partial and full contributions\n",
    "    enumerate_counter = 0\n",
    "    for data, contrib_type in [(partial_data, 'partial'), (full_data, 'full')]:\n",
    "        # Compute percentiles along the first axis (runs)\n",
    "        p20 = np.percentile(data, 20, axis=0)\n",
    "        p50 = np.percentile(data, 50, axis=0)\n",
    "        p80 = np.percentile(data, 80, axis=0)\n",
    "        \n",
    "        # Create x-axis (time steps)\n",
    "        x = np.arange(data.shape[1])\n",
    "        \n",
    "        # Plot for each dimension (training/validation)\n",
    "        for dim in range(2):\n",
    "            plt.plot(x, p50[:, dim], color=colors[contrib_type], \n",
    "                    label=f'{contrib_type.capitalize()} {labels[dim]} Median', \n",
    "                    linestyle=linestyles[dim], linewidth=1.0)\n",
    "            if dim == 0:\n",
    "                legend_entries.append(legend_list[enumerate_counter])\n",
    "            else:\n",
    "                legend_entries.append(f\"_\")\n",
    "\n",
    "            plt.fill_between(x, p20[:, dim], p80[:, dim], \n",
    "                           alpha=0.2, color=colors[contrib_type])\n",
    "            legend_entries.append(f\"_\")\n",
    "\n",
    "            enumerate_counter += 1\n",
    "    \n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend(loc='upper left', labels=legend_entries)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # path_to_save = f\"../results/plots/training_curve/{dataset}.pdf\"\n",
    "    # directory = os.path.dirname(path_to_save)\n",
    "    # if not os.path.exists(directory):\n",
    "    #     os.makedirs(directory)\n",
    "        \n",
    "    # plt.savefig(path_to_save, dpi=300)\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa6324d3-4b46-4304-91ce-3015da65f457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified main loop\n",
    "# dataset_list = [\"alternate_stripes\", \"circles\", \"spheres\", \"wine\", \"heart-statlog\", \"ionosphere\", \"breast-cancer\"]\n",
    "dataset_list = [\"german_credit\"]\n",
    "for dataset in dataset_list:\n",
    "    activation = \"relu\"\n",
    "    data_dictionary = load_data(dataset)\n",
    "    if dataset == \"alternate_stripes\":\n",
    "        activation = \"cos\"\n",
    "    \n",
    "    partial_contrib_data = [value for key, value in data_dictionary.items() \n",
    "                          if 'batch_norm=true/partial_contrib=true' in key and activation in key]\n",
    "    full_contrib_data = [value for key, value in data_dictionary.items() \n",
    "                        if 'batch_norm=true/partial_contrib=false' in key and activation in key]\n",
    "    \n",
    "    explained_variance_partial = extract_explained_variance(partial_contrib_data)\n",
    "    explained_variance_full = extract_explained_variance(full_contrib_data)\n",
    "    \n",
    "    # Plot both contributions on the same figure\n",
    "    plot_percentiles(explained_variance_partial, explained_variance_full, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f55c0e41-f645-4204-b7f7-0df2667bcae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.08470224, 0.08901697],\n",
       "        [0.08470219, 0.08900488],\n",
       "        [0.08471598, 0.089025  ],\n",
       "        ...,\n",
       "        [0.14570874, 0.14225343],\n",
       "        [0.14570676, 0.14226034],\n",
       "        [0.14571006, 0.1422589 ]],\n",
       "\n",
       "       [[0.09725811, 0.10068547],\n",
       "        [0.09744301, 0.10088969],\n",
       "        [0.09763142, 0.10109312],\n",
       "        ...,\n",
       "        [0.14541388, 0.13838832],\n",
       "        [0.14541624, 0.13839281],\n",
       "        [0.14541663, 0.13838634]],\n",
       "\n",
       "       [[0.09257888, 0.10184685],\n",
       "        [0.09259284, 0.10196629],\n",
       "        [0.09259981, 0.10206835],\n",
       "        ...,\n",
       "        [0.13682394, 0.14548321],\n",
       "        [0.13682383, 0.14548826],\n",
       "        [0.13682619, 0.14549922]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.09931993, 0.092513  ],\n",
       "        [0.09940549, 0.09258169],\n",
       "        [0.09947887, 0.092637  ],\n",
       "        ...,\n",
       "        [0.1319326 , 0.11496097],\n",
       "        [0.13193772, 0.11498557],\n",
       "        [0.13193979, 0.11498569]],\n",
       "\n",
       "       [[0.09785248, 0.10005312],\n",
       "        [0.09787641, 0.10006712],\n",
       "        [0.0978929 , 0.10007904],\n",
       "        ...,\n",
       "        [0.1417047 , 0.12939958],\n",
       "        [0.14170838, 0.1294005 ],\n",
       "        [0.14171248, 0.12939839]],\n",
       "\n",
       "       [[0.08763331, 0.06836516],\n",
       "        [0.08766697, 0.06847911],\n",
       "        [0.08770278, 0.0686018 ],\n",
       "        ...,\n",
       "        [0.14412079, 0.15836275],\n",
       "        [0.14412165, 0.15836199],\n",
       "        [0.14412275, 0.15836897]]], shape=(15, 10000, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_variance_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2647560d-a17f-4f6e-a2cf-6633371112c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ba4a2-0c4d-4a68-9e07-2a3e01c6ef5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ce457-2859-4871-8af0-f442c019da5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c51ee9-e366-4962-abbd-f38a3fb0f0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273943fe-06b5-42cc-96ad-cd029525fa27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
