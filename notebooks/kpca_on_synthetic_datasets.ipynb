{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import math\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "import yaml\n",
    "from scipy.io import arff\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import scale, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# synthetic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_alternate_stripes():\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for i in [np.pi * i for i in [-3, -1, 1, 3, 5]]:\n",
    "        noise = np.random.randn(200, 1)*0.1\n",
    "        x = np.expand_dims(np.repeat(i, 200), axis=1) + noise\n",
    "        y = noise\n",
    "\n",
    "        concat = np.concatenate((x, y), axis=1)\n",
    "        data.append(concat)\n",
    "\n",
    "    for i in [np.pi * i for i in [-4, -2, 0, 2, 4]]:\n",
    "        noise = np.random.randn(200, 1)*0.1\n",
    "        x = np.expand_dims(np.repeat(i, 200), axis=1) + noise\n",
    "        y = noise\n",
    "\n",
    "        concat = np.concatenate((x, y), axis=1)\n",
    "        data.append(concat)\n",
    "\n",
    "    data_x = np.reshape(np.array(data), (2000, 2))\n",
    "\n",
    "    return data_x\n",
    "\n",
    "\n",
    "def fibonacci_sphere(r, samples=1000, mu=0, sigma=0.05):\n",
    "    points = []\n",
    "    phi = math.pi * (3. - math.sqrt(5.))  # golden angle in radians\n",
    "\n",
    "    for i in range(samples):\n",
    "        y = 1 - (i / float(samples - 1)) * 2  # y goes from 1 to -1\n",
    "        radius = math.sqrt(1 - y * y)  # radius at y\n",
    "\n",
    "        theta = phi * i  # golden angle increment\n",
    "\n",
    "        x = math.cos(theta) * radius\n",
    "        z = math.sin(theta) * radius\n",
    "\n",
    "        points.append((x, y, z))\n",
    "\n",
    "        noise = np.random.randn(1000, 3)\n",
    "        noise = noise * sigma + mu\n",
    "\n",
    "    return np.array(points) * r + noise\n",
    "\n",
    "\n",
    "def make_two_spheres():\n",
    "    sphere_one = fibonacci_sphere(0.1)\n",
    "    sphere_two = fibonacci_sphere(0.5)\n",
    "\n",
    "    x = np.concatenate((sphere_one, sphere_two), axis=0)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def circles_data(x0=0, y0=0):\n",
    "    np.random.seed(0)\n",
    "    x, y = datasets.make_circles(n_samples=2000, factor=0.1, noise=0.05)\n",
    "    x[:, 0] += x0\n",
    "    x[:, 1] += y0\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigDataset(BaseModel):\n",
    "    num_features: int\n",
    "    synthetic_bool: bool\n",
    "    categorical_features: Optional[List[int]] = Field(default_factory=list)\n",
    "    numerical_features: Optional[List[int]] = Field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_config_load(dataset: str) -> ConfigDataset:\n",
    "    \"\"\"load config file into pydantic object\"\"\"\n",
    "    file_path = \"../datasets_config.yaml\"\n",
    "    \n",
    "    with open(file_path) as file:\n",
    "        config_data = yaml.safe_load(file)\n",
    "        config_data = config_data[dataset]\n",
    "\n",
    "    return ConfigDataset(**config_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_arff(path):\n",
    "    data, meta = arff.loadarff(path)\n",
    "    data = pd.DataFrame(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_data(data: pd.DataFrame, dataset: str) -> tuple[pd.DataFrame, np.array]:\n",
    "    type_class = np.zeros(shape=(1, data.shape[0]))\n",
    "    if dataset == \"abalone\":\n",
    "        type_class = data[\"sex\"]\n",
    "\n",
    "    elif dataset in [\"phoneme\", \"breast_cancer\"]:\n",
    "        type_class = data[\"Class\"]\n",
    "\n",
    "    elif dataset in [\"wine\", \"ionosphere\", \"german_credit\", \"dermatology\", \"heart-statlog\"]:\n",
    "        type_class = data[\"class\"]\n",
    "\n",
    "    return data, type_class\n",
    "\n",
    "\n",
    "def load_data(dataset: str) -> pd.DataFrame:\n",
    "    if dataset == \"spheres\":\n",
    "        data = make_two_spheres()\n",
    "\n",
    "    elif dataset == \"circles\":\n",
    "        data = circles_data()\n",
    "\n",
    "    elif dataset == \"alternate_stripes\":\n",
    "        data = make_alternate_stripes()\n",
    "\n",
    "    else:\n",
    "        path = f\"../datasets/{dataset}.arff\"\n",
    "        data = read_arff(path)\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_indices(data: np.array, random_seed: int, val_prop: float = 0.2) -> tuple[np.array, np.array]:\n",
    "    np.random.seed(random_seed)\n",
    "    n = data.shape[0]\n",
    "    indices = np.arange(n)\n",
    "    np.random.shuffle(indices)\n",
    "    train_indices = indices[int(n * val_prop):]\n",
    "    val_indices = indices[:int(n * val_prop)]\n",
    "\n",
    "    return train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_onehot(data: pd.DataFrame, object_indices: list[int]) -> tuple[pd.DataFrame, list]:\n",
    "    object_indices = np.array(object_indices)\n",
    "    data_to_one_hot = data.iloc[:, object_indices]\n",
    "\n",
    "    num_cols_per_categories = list(data_to_one_hot.nunique())\n",
    "    cols_to_remove = data_to_one_hot.columns\n",
    "\n",
    "    data_to_one_hot = pd.get_dummies(data_to_one_hot, columns=cols_to_remove, dtype=int)\n",
    "\n",
    "    data = data.drop(columns=cols_to_remove, inplace=False)\n",
    "    num_cols_per_categories = num_cols_per_categories + [1] * data.shape[1]\n",
    "    data = pd.concat((data_to_one_hot, data), axis=1)\n",
    "\n",
    "    return data, num_cols_per_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_preprocess(dataset: str):\n",
    "\n",
    "    dataset_config = dataset_config_load(dataset)\n",
    "    x = load_data(dataset)\n",
    "    x = x.dropna()\n",
    "\n",
    "    classes = np.zeros(x.shape[0])\n",
    "\n",
    "    if dataset not in [\"circles\", \"spheres\", \"alternate_stripes\"]:\n",
    "        x, classes = preprocess_data(x, dataset)\n",
    "        classes, mapping = pd.factorize(classes)\n",
    "\n",
    "    # transform categorical columns to one-hot encoded.\n",
    "    if dataset_config.categorical_features:\n",
    "        x, num_features_per_network = transform_data_onehot(x,\n",
    "                                                            dataset_config.categorical_features\n",
    "                                                            )\n",
    "    else:\n",
    "        num_features_per_network = np.array([1] * x.shape[1])\n",
    "\n",
    "    train_indices, val_indices = get_split_indices(x, run_index)\n",
    "    train_x = x.iloc[train_indices].values\n",
    "    val_x = x.iloc[val_indices].values\n",
    "\n",
    "    return train_x, val_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA and plot the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = False\n",
    "kernel_list = [\"rbf\", \"cosine\", \"poly\", \"sigmoid\"]\n",
    "\n",
    "dataset_list = [\"alternate_stripes\",\n",
    "                \"circles\",\n",
    "                \"spheres\", \n",
    "                \"wine\", \n",
    "                \"heart-statlog\", \n",
    "                \"ionosphere\", \n",
    "                \"breast_cancer\", \n",
    "                \"german_credit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing for dataset alternate_stripes...\n",
      "Computing for kernel rbf...\n",
      "Computing for kernel cosine...\n",
      "Computing for kernel poly...\n",
      "Computing for kernel sigmoid...\n",
      "Computing for dataset circles...\n",
      "Computing for kernel rbf...\n",
      "Computing for kernel cosine...\n",
      "Computing for kernel poly...\n",
      "Computing for kernel sigmoid...\n",
      "Computing for dataset spheres...\n",
      "Computing for kernel rbf...\n",
      "Computing for kernel cosine...\n",
      "Computing for kernel poly...\n",
      "Computing for kernel sigmoid...\n",
      "Computing for dataset wine...\n",
      "Computing for kernel rbf...\n",
      "Computing for kernel cosine...\n",
      "Computing for kernel poly...\n",
      "Computing for kernel sigmoid...\n",
      "Computing for dataset heart-statlog...\n",
      "Computing for kernel rbf...\n",
      "Computing for kernel cosine...\n",
      "Computing for kernel poly...\n",
      "Computing for kernel sigmoid...\n",
      "Computing for dataset ionosphere...\n",
      "Computing for kernel rbf...\n",
      "Computing for kernel cosine...\n",
      "Computing for kernel poly...\n",
      "Computing for kernel sigmoid...\n",
      "Computing for dataset breast_cancer...\n",
      "Computing for kernel rbf...\n",
      "Computing for kernel cosine...\n",
      "Computing for kernel poly...\n",
      "Computing for kernel sigmoid...\n",
      "Computing for dataset german_credit...\n",
      "Computing for kernel rbf...\n",
      "Computing for kernel cosine...\n",
      "Computing for kernel poly...\n",
      "Computing for kernel sigmoid...\n"
     ]
    }
   ],
   "source": [
    "data_dict = {}\n",
    "\n",
    "for dataset in dataset_list:\n",
    "    print(f\"Computing for dataset {dataset}...\")\n",
    "    data_dict[dataset] = {}\n",
    "    \n",
    "    for ke in kernel_list:\n",
    "        print(f\"Computing for kernel {ke}...\")\n",
    "        data_dict[dataset][ke] = []\n",
    "        for run_index in range(15):\n",
    "    \n",
    "            train_x, val_x = load_data_and_preprocess(dataset)\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(train_x)\n",
    "            train_x = scaler.transform(train_x)\n",
    "            val_x = scaler.transform(val_x)\n",
    "            \n",
    "            kernel_pca = KernelPCA(n_components=train_x.shape[1], kernel=ke, gamma=1, alpha=0.1, degree=2)\n",
    "            kernel_pca.fit(train_x)\n",
    "            X_kpca = kernel_pca.transform(train_x)\n",
    "            X_kpca_val = kernel_pca.transform(val_x)\n",
    "    \n",
    "            if plot:\n",
    "                # Plot results\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.grid(True)\n",
    "            \n",
    "                ax.scatter(X_kpca[:, 0], X_kpca[:, 1], c=\"blue\", s=20, edgecolor=\"k\")\n",
    "                low_y_lim, high_y_lim = plt.gca().get_ylim()\n",
    "                low_x_lim, high_x_lim = plt.gca().get_xlim()\n",
    "                ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "                ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "                ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "                ax.set_xlabel(r\"$\\widetilde{z}_1$\", size=15)\n",
    "                ax.set_ylabel(r\"$\\widetilde{z}_2$\", size=15)\n",
    "                plt.savefig(f'original_{dataset}_{ke}.pdf', dpi=300, bbox_inches='tight')\n",
    "                plt.show()\n",
    "    \n",
    "            training_explained_var = kernel_pca.eigenvalues_/np.sum(kernel_pca.eigenvalues_)\n",
    "            # For training data, compute variance along each component. Check that this is the same as the above => it is.\n",
    "            # train_var = np.var(X_kpca, axis=0)\n",
    "            # train_exp_var_ratio = train_var / np.sum(train_var)\n",
    "            # print(\"Validation projected explained variance ratio:\", train_exp_var_ratio)\n",
    "    \n",
    "            # For validation data, compute variance along each component\n",
    "            val_var = np.var(X_kpca_val, axis=0)\n",
    "            val_exp_var_ratio = val_var / np.sum(val_var)\n",
    "            # print(\"Validation projected explained variance ratio:\", val_exp_var_ratio)\n",
    "    \n",
    "            data_dict[dataset][ke].append((training_explained_var, val_exp_var_ratio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
