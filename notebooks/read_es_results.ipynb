{
 "cells": [
  {
   "cell_type": "code",
   "id": "d5741067-39ef-4311-a4ca-6a3a1ac86f84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T21:46:17.255052Z",
     "start_time": "2026-01-23T21:46:17.252488Z"
    }
   },
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import os\n",
    "from typing import Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T21:46:17.709787Z",
     "start_time": "2026-01-23T21:46:17.706802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ],
   "id": "c481d6422f1b6ef2",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "4dcbba6e-0b51-4783-8764-fd5bc19dfb2b",
   "metadata": {},
   "source": [
    "# load results"
   ]
  },
  {
   "cell_type": "code",
   "id": "4300fed5-f869-4240-a3db-5091b2d11171",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T21:46:18.227843Z",
     "start_time": "2026-01-23T21:46:18.224232Z"
    }
   },
   "source": [
    "# pydantic model\n",
    "\n",
    "class PreprocessingSteps(BaseModel):\n",
    "    transformers: Tuple[\n",
    "        Tuple[PCA, StandardScaler, np.ndarray, np.ndarray],\n",
    "        Tuple[float, float]\n",
    "    ] = Field(\n",
    "        ..., \n",
    "        description=\"Preprocessing steps including PCA, StandardScaler, and arrays\"\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "        json_encoders = {\n",
    "            np.ndarray: lambda v: v.tolist(),\n",
    "            PCA: lambda v: str(v),\n",
    "            StandardScaler: lambda v: str(v)\n",
    "        }\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "c3299088-7b44-4250-9db7-00d2317df289",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T21:46:18.592930Z",
     "start_time": "2026-01-23T21:46:18.589624Z"
    }
   },
   "source": [
    "def load_data(dataset: str, activation: str = \"relu\") -> dict:\n",
    "    \"\"\"\n",
    "    Load data from pickle files for a specific dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset (str): Name of the dataset or specific subdirectory\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with full file paths as keys and loaded data as values\n",
    "    \"\"\"\n",
    "    \n",
    "    base_dir = \"../results/datasets/real_world_data/\"\n",
    "    if dataset in [\"circles\", \"spheres\", \"alternate_stripes\"]:\n",
    "        base_dir = \"../results/datasets/synthetic_data\"\n",
    "    \n",
    "    full_path = os.path.join(base_dir, dataset)\n",
    "    full_path = full_path + f\"/activation={activation}\"\n",
    "\n",
    "    print(full_path)\n",
    "    \n",
    "    data_dict = {}\n",
    "    \n",
    "    # Walk through the directory tree starting from the specified dataset path\n",
    "    for root, dirs, files in os.walk(full_path):\n",
    "        for file in files:\n",
    "            # Check if the file is a pickle file\n",
    "            if file.endswith('.pkl') or file.endswith('.p'):\n",
    "                full_file_path = os.path.join(root, file)\n",
    "                \n",
    "                try:\n",
    "                    # Load the pickle file\n",
    "                    with open(full_file_path, \"rb\") as f:\n",
    "                        data = pickle.load(f)\n",
    "                    \n",
    "                    # Store in the dictionary with full path as key\n",
    "                    data_dict[full_file_path] = data\n",
    "                \n",
    "                except (IOError, pickle.UnpicklingError) as e:\n",
    "                    print(f\"Error loading {full_file_path}: {e}\")\n",
    "    \n",
    "    return data_dict"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "362faac8-fe45-41ed-9d17-bc644f543476",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T21:46:18.992904Z",
     "start_time": "2026-01-23T21:46:18.990776Z"
    }
   },
   "source": [
    "def read_all_nested_files(dataset: str, activation: str = \"relu\", k: int = 2):\n",
    "    base_dir = \"../results/datasets/real_world_data/\"\n",
    "    if dataset in [\"circles\", \"spheres\", \"alternate_stripes\"]:\n",
    "        base_dir = \"../results/datasets/synthetic_data\"\n",
    "\n",
    "    # NEW STRUCTURE: .../<dataset>/k=<k>/activation=<activation>/...\n",
    "    full_path = os.path.join(base_dir, dataset, f\"k={k}\", f\"activation={activation}\")\n",
    "\n",
    "    file_contents = {}\n",
    "    for root, dirs, files in os.walk(full_path):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "\n",
    "            # For training curves, results_list.p is the one you want\n",
    "            if os.path.basename(file_path) == \"results_list.p\":\n",
    "                try:\n",
    "                    with open(file_path, \"rb\") as f:\n",
    "                        file_contents[file_path] = pickle.load(f)\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not read {file_path}: {e}\")\n",
    "\n",
    "    return file_contents\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "8b57422d-a837-4228-a4b2-6c71bb96cf18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T21:46:19.184555Z",
     "start_time": "2026-01-23T21:46:19.182831Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d775b7de-f7e9-48be-8171-367dff4f2e80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T21:46:19.407387Z",
     "start_time": "2026-01-23T21:46:19.405436Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c6ca4b75-dad1-4de5-abab-cf3d58b8fb79",
   "metadata": {},
   "source": [
    "# functions to collate the data"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T21:46:21.187840Z",
     "start_time": "2026-01-23T21:46:21.183244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get the explained variance objectives for train and validation\n",
    "\n",
    "def extract_explained_variance(data: list[list[PreprocessingSteps]]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extracts the explained variance curves from a list of runs and returns a\n",
    "    padded array with NaNs for variable-length runs.\n",
    "\n",
    "    Input structure expectation (per run):\n",
    "    - data[j] is an iterable of time steps for run j\n",
    "    - each element has the explained variance as the last item (index -1)\n",
    "      and is typically a 1D array-like of length 2: [train, val]\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray with shape (n_runs, max_time, d), padded with np.nan\n",
    "      so that ragged runs are handled consistently downstream.\n",
    "    \"\"\"\n",
    "\n",
    "    runs: list[np.ndarray] = []\n",
    "    max_len = 0\n",
    "\n",
    "    for run in data:\n",
    "        # Collect per-step explained variance for this run\n",
    "        seq = []\n",
    "        for step in range(len(run)):\n",
    "            try:\n",
    "                last = run[step][-1]\n",
    "            except Exception:\n",
    "                # If structure is unexpected, skip this step\n",
    "                continue\n",
    "\n",
    "            arr = np.array(last)\n",
    "            # Ensure 1D shape\n",
    "            if arr.ndim == 0:\n",
    "                arr = np.array([float(arr)])\n",
    "            elif arr.ndim > 1:\n",
    "                arr = arr.ravel()\n",
    "\n",
    "            seq.append(arr.astype(float, copy=False))\n",
    "\n",
    "        if len(seq) == 0:\n",
    "            continue\n",
    "\n",
    "        seq_arr = np.vstack([s if s.ndim == 1 else s.ravel() for s in seq])\n",
    "        runs.append(seq_arr)\n",
    "        max_len = max(max_len, seq_arr.shape[0])\n",
    "\n",
    "    if len(runs) == 0:\n",
    "        return np.empty((0, 0, 0))\n",
    "\n",
    "    # Infer feature dimension (e.g., 2 for [train, val])\n",
    "    d = runs[0].shape[1] if runs[0].ndim == 2 else 1\n",
    "\n",
    "    padded = np.full((len(runs), max_len, d), np.nan, dtype=float)\n",
    "    for i, r in enumerate(runs):\n",
    "        T = r.shape[0]\n",
    "        if r.ndim == 1:\n",
    "            padded[i, :T, 0] = r\n",
    "        else:\n",
    "            dd = min(d, r.shape[1])\n",
    "            padded[i, :T, :dd] = r[:, :dd]\n",
    "\n",
    "    return padded"
   ],
   "id": "da6f2651d72508af",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T21:46:21.644095Z",
     "start_time": "2026-01-23T21:46:21.642187Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ee24244e21106b03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# visualise",
   "id": "c896927b580dbc8d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T21:46:30.229595Z",
     "start_time": "2026-01-23T21:46:23.301957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_percentiles(\n",
    "        partial_data_by_k: dict[int, np.ndarray],\n",
    "        full_data_by_k: dict[int, np.ndarray],\n",
    "        dataset: str,\n",
    "        ks: tuple[int, ...] = (1, 2),\n",
    "        activation: str | None = None,\n",
    "        ax: plt.Axes | None = None,\n",
    "        y_n_ticks: int = 5,\n",
    "):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Colors for different contribution types\n",
    "    colors = {\"partial\": \"darkorange\", \"full\": \"darkblue\"}\n",
    "    linestyles = {1: \"--\", 2: \"-\"}  # dashed for k=1, solid for k=2\n",
    "\n",
    "    if activation is None:\n",
    "        activation_fn = \"cos\" if dataset == \"alternate_stripes\" else \"relu\"\n",
    "    else:\n",
    "        activation_fn = activation\n",
    "\n",
    "    for k in ks:\n",
    "        for data_by_k, contrib_type in [(partial_data_by_k, \"partial\"), (full_data_by_k, \"full\")]:\n",
    "            data = data_by_k.get(k, None)\n",
    "\n",
    "            if data is None or not hasattr(data, \"size\") or data.size == 0 or data.shape[1] == 0:\n",
    "                print(f\"FOR k={k}, contrib type {contrib_type} -> no data to plot\")\n",
    "                continue\n",
    "\n",
    "            # Compute percentiles ignoring NaNs\n",
    "            p20 = np.nanpercentile(data, 20, axis=0)\n",
    "            p50 = np.nanpercentile(data, 50, axis=0)\n",
    "            p80 = np.nanpercentile(data, 80, axis=0)\n",
    "\n",
    "            valid_mask = ~np.all(np.isnan(data[:, :, 0]), axis=0)\n",
    "            x = np.arange(data.shape[1])[valid_mask]\n",
    "\n",
    "            D = p50.shape[1] if p50.ndim == 2 else 1\n",
    "            val_dim = 1 if D > 1 else 0\n",
    "\n",
    "            y = p50[valid_mask, val_dim] if D > 1 else p50[valid_mask]\n",
    "            ax.plot(\n",
    "                x,\n",
    "                y,\n",
    "                color=colors[contrib_type],\n",
    "                linestyle=linestyles.get(k, \"-\"),\n",
    "                linewidth=2.2,\n",
    "            )\n",
    "\n",
    "            lower = p20[valid_mask, val_dim] if D > 1 else p20[valid_mask]\n",
    "            upper = p80[valid_mask, val_dim] if D > 1 else p80[valid_mask]\n",
    "            ax.fill_between(x, lower, upper, alpha=0.3, color=colors[contrib_type])\n",
    "\n",
    "    ax.set_title(dataset, fontsize=14)\n",
    "    ax.set_xlabel(\"Generations\", fontsize=12)\n",
    "    ax.set_ylabel(r\"Proportion of explained variance\", fontsize=12)\n",
    "    ax.tick_params(axis=\"both\", labelsize=11)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=y_n_ticks))\n",
    "\n",
    "    return activation_fn\n",
    "\n",
    "# -------------------------------\n",
    "# Main plotting workflow\n",
    "# -------------------------------\n",
    "dataset_list = [\n",
    "    \"alternate_stripes\",\n",
    "    \"circles\",\n",
    "    \"spheres\",\n",
    "    \"wine\",\n",
    "    \"heart-statlog\",\n",
    "    \"ionosphere\",\n",
    "    \"breast_cancer\",\n",
    "    \"german_credit\",\n",
    "]\n",
    "\n",
    "ks_to_plot = (1, 2)\n",
    "y_n_ticks = 5  # number of ticks per subplot\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(14, 18), sharey=False)\n",
    "axes = axes.ravel()\n",
    "fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "\n",
    "for i, dataset in enumerate(dataset_list):\n",
    "    ax = axes[i]\n",
    "    activation = \"cos\" if dataset == \"alternate_stripes\" else \"relu\"\n",
    "\n",
    "    ks_for_dataset = (1,) if dataset == \"spheres\" else ks_to_plot\n",
    "\n",
    "    partial_by_k: dict[int, np.ndarray] = {}\n",
    "    full_by_k: dict[int, np.ndarray] = {}\n",
    "\n",
    "    # -------------------------------\n",
    "    # Load data for each k\n",
    "    # -------------------------------\n",
    "    for k in ks_for_dataset:\n",
    "        data_dictionary = read_all_nested_files(dataset, activation=activation, k=k)\n",
    "\n",
    "        partial_contrib_data = [\n",
    "            value for key, value in data_dictionary.items()\n",
    "            if \"partial_contrib=True\" in key and key.endswith(os.path.sep + \"results_list.p\")\n",
    "        ]\n",
    "\n",
    "        full_contrib_data = [\n",
    "            value for key, value in data_dictionary.items()\n",
    "            if \"partial_contrib=False\" in key and key.endswith(os.path.sep + \"results_list.p\")\n",
    "        ]\n",
    "\n",
    "        partial_by_k[k] = extract_explained_variance(partial_contrib_data)\n",
    "        full_by_k[k] = extract_explained_variance(full_contrib_data)\n",
    "\n",
    "        # -------------------------------\n",
    "        # Compute and print mean/std for last generation\n",
    "        # -------------------------------\n",
    "        for contrib_type, data in [(\"Partial\", partial_by_k[k]), (\"Global\", full_by_k[k])]:\n",
    "            if data is None or data.size == 0:\n",
    "                continue\n",
    "            last_gen_idx = data.shape[1] - 1\n",
    "            last_gen_data = data[:, last_gen_idx, ...]\n",
    "            mean_val = np.nanmean(last_gen_data)\n",
    "            std_val = np.nanstd(last_gen_data)\n",
    "            print(\n",
    "                f\"Dataset={dataset}, k={k}, contrib={contrib_type}, \"\n",
    "                f\"last generation mean={mean_val:.4f}, std={std_val:.4f}\"\n",
    "            )\n",
    "\n",
    "    # -------------------------------\n",
    "    # Plot percentiles\n",
    "    # -------------------------------\n",
    "    plot_percentiles(\n",
    "        partial_by_k,\n",
    "        full_by_k,\n",
    "        dataset,\n",
    "        ks=ks_for_dataset,\n",
    "        activation=activation,\n",
    "        ax=ax,\n",
    "        y_n_ticks=y_n_ticks,\n",
    "    )\n",
    "\n",
    "# -------------------------------\n",
    "# Adjust y-axis: fixed number of ticks per subplot\n",
    "# -------------------------------\n",
    "for ax in axes:\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    ticks = np.linspace(ymin, ymax, y_n_ticks)  # evenly spaced including bottom/top\n",
    "    ticks = [np.round(val, 2) for val in ticks]\n",
    "    ax.set_yticks(ticks)\n",
    "\n",
    "# -------------------------------\n",
    "# Create beautiful custom legend\n",
    "# -------------------------------\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Define colors and linestyles\n",
    "colors = {\"Partial\": \"darkorange\", \"Full\": \"darkblue\"}\n",
    "linestyles = {\"k=1\": \"--\", \"k=2\": \"-\"}\n",
    "\n",
    "# Create custom legend handles\n",
    "legend_elements = []\n",
    "for contrib_type in [\"Partial\", \"Full\"]:\n",
    "    for k_label in [\"k=1\", \"k=2\"]:\n",
    "        legend_elements.append(\n",
    "            Line2D([0], [0],\n",
    "                   color=colors[contrib_type],\n",
    "                   linestyle=linestyles[k_label],\n",
    "                   linewidth=2.2,\n",
    "                   label=f\"{contrib_type}, {k_label}\")\n",
    "        )\n",
    "\n",
    "# Add legend above the plots in 2x2 layout\n",
    "fig.legend(\n",
    "    handles=legend_elements,\n",
    "    loc=\"upper center\",\n",
    "    ncol=2,\n",
    "    fontsize=11,\n",
    "    frameon=True,\n",
    "    fancybox=True,\n",
    "    shadow=True,\n",
    "    bbox_to_anchor=(0.5, 0.99)\n",
    ")\n",
    "\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "fig.savefig(\"multi_dataset_plot.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close(fig)\n"
   ],
   "id": "a8ef7f1085140b08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset=alternate_stripes, k=1, contrib=Partial, last generation mean=0.6934, std=0.1035\n",
      "Dataset=alternate_stripes, k=1, contrib=Global, last generation mean=0.7150, std=0.0730\n",
      "FOR k=2, contrib type partial -> no data to plot\n",
      "FOR k=2, contrib type full -> no data to plot\n",
      "Dataset=circles, k=1, contrib=Partial, last generation mean=0.7356, std=0.0775\n",
      "Dataset=circles, k=1, contrib=Global, last generation mean=0.7141, std=0.0734\n",
      "FOR k=2, contrib type partial -> no data to plot\n",
      "FOR k=2, contrib type full -> no data to plot\n",
      "Dataset=spheres, k=1, contrib=Partial, last generation mean=0.5956, std=0.0208\n",
      "Dataset=spheres, k=1, contrib=Global, last generation mean=0.5174, std=0.0391\n",
      "Dataset=wine, k=1, contrib=Partial, last generation mean=0.4478, std=0.0279\n",
      "Dataset=wine, k=1, contrib=Global, last generation mean=0.3635, std=0.0345\n",
      "Dataset=wine, k=2, contrib=Partial, last generation mean=0.6203, std=0.0216\n",
      "Dataset=wine, k=2, contrib=Global, last generation mean=0.5436, std=0.0279\n",
      "Dataset=heart-statlog, k=1, contrib=Partial, last generation mean=0.2677, std=0.0225\n",
      "Dataset=heart-statlog, k=1, contrib=Global, last generation mean=0.2413, std=0.0247\n",
      "Dataset=heart-statlog, k=2, contrib=Partial, last generation mean=0.3808, std=0.0257\n",
      "Dataset=heart-statlog, k=2, contrib=Global, last generation mean=0.3449, std=0.0300\n",
      "Dataset=ionosphere, k=1, contrib=Partial, last generation mean=0.3247, std=0.0616\n",
      "Dataset=ionosphere, k=1, contrib=Global, last generation mean=0.2384, std=0.0191\n",
      "Dataset=ionosphere, k=2, contrib=Partial, last generation mean=0.5431, std=0.0287\n",
      "Dataset=ionosphere, k=2, contrib=Global, last generation mean=0.3987, std=0.0331\n",
      "Dataset=breast_cancer, k=1, contrib=Partial, last generation mean=0.2454, std=0.0282\n",
      "Dataset=breast_cancer, k=1, contrib=Global, last generation mean=0.2312, std=0.0248\n",
      "Dataset=breast_cancer, k=2, contrib=Partial, last generation mean=0.4184, std=0.0268\n",
      "Dataset=breast_cancer, k=2, contrib=Global, last generation mean=0.3747, std=0.0338\n",
      "Dataset=german_credit, k=1, contrib=Partial, last generation mean=0.1527, std=0.0079\n",
      "Dataset=german_credit, k=1, contrib=Global, last generation mean=0.1187, std=0.0115\n",
      "Dataset=german_credit, k=2, contrib=Partial, last generation mean=0.2528, std=0.0107\n",
      "Dataset=german_credit, k=2, contrib=Global, last generation mean=0.2097, std=0.0128\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "61809b86-07da-4cbf-8844-1808dc625f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T20:52:47.707735Z",
     "start_time": "2026-01-23T20:52:47.706445Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64d1ce38-d683-43f1-bde6-62fa5ac13574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T20:52:47.711243Z",
     "start_time": "2026-01-23T20:52:47.710076Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f50303d3-cc38-44f9-bb16-d61b3765ba2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T20:52:47.714785Z",
     "start_time": "2026-01-23T20:52:47.713585Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ce457-2859-4871-8af0-f442c019da5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c51ee9-e366-4962-abbd-f38a3fb0f0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273943fe-06b5-42cc-96ad-cd029525fa27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
