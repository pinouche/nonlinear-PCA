{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5741067-39ef-4311-a4ca-6a3a1ac86f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from typing import Tuple, List\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcbba6e-0b51-4783-8764-fd5bc19dfb2b",
   "metadata": {},
   "source": [
    "# load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4300fed5-f869-4240-a3db-5091b2d11171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydantic model\n",
    "\n",
    "class PreprocessingSteps(BaseModel):\n",
    "    transformers: Tuple[\n",
    "        Tuple[PCA, StandardScaler, np.ndarray, np.ndarray],\n",
    "        Tuple[float, float]\n",
    "    ] = Field(\n",
    "        ..., \n",
    "        description=\"Preprocessing steps including PCA, StandardScaler, and arrays\"\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "        json_encoders = {\n",
    "            np.ndarray: lambda v: v.tolist(),\n",
    "            PCA: lambda v: str(v),\n",
    "            StandardScaler: lambda v: str(v)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3299088-7b44-4250-9db7-00d2317df289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset: str, activation: str = \"relu\") -> dict:\n",
    "    \"\"\"\n",
    "    Load data from pickle files for a specific dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset (str): Name of the dataset or specific subdirectory\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with full file paths as keys and loaded data as values\n",
    "    \"\"\"\n",
    "    \n",
    "    base_dir = \"../results/datasets/real_world_data/\"\n",
    "    if dataset in [\"circles\", \"spheres\", \"alternate_stripes\"]:\n",
    "        base_dir = \"../results/datasets/synthetic_data\"\n",
    "    \n",
    "    full_path = os.path.join(base_dir, dataset)\n",
    "    full_path = full_path + f\"/activation={activation}\"\n",
    "\n",
    "    print(full_path)\n",
    "    \n",
    "    data_dict = {}\n",
    "    \n",
    "    # Walk through the directory tree starting from the specified dataset path\n",
    "    for root, dirs, files in os.walk(full_path):\n",
    "        for file in files:\n",
    "            # Check if the file is a pickle file\n",
    "            if file.endswith('.pkl') or file.endswith('.p'):\n",
    "                full_file_path = os.path.join(root, file)\n",
    "                \n",
    "                try:\n",
    "                    # Load the pickle file\n",
    "                    with open(full_file_path, \"rb\") as f:\n",
    "                        data = pickle.load(f)\n",
    "                    \n",
    "                    # Store in the dictionary with full path as key\n",
    "                    data_dict[full_file_path] = data\n",
    "                \n",
    "                except (IOError, pickle.UnpicklingError) as e:\n",
    "                    print(f\"Error loading {full_file_path}: {e}\")\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "362faac8-fe45-41ed-9d17-bc644f543476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_nested_files(dataset: str, activation: str = \"relu\"):\n",
    "\n",
    "    base_dir = \"../results/datasets/real_world_data/\"\n",
    "    if dataset in [\"circles\", \"spheres\", \"alternate_stripes\"]:\n",
    "        base_dir = \"../results/datasets/synthetic_data\"\n",
    "    \n",
    "    full_path = os.path.join(base_dir, dataset)\n",
    "    full_path = full_path + f\"/activation={activation}\"\n",
    "    \n",
    "    # Dictionary to store file contents with file paths as keys\n",
    "    file_contents = {}\n",
    "    \n",
    "    # Walk through all directories and files\n",
    "    for root, dirs, files in os.walk(full_path):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            if \"results_list.p\" in file_path or re.search(r'^[^a-zA-Z]*\\d+\\.p', file_path.split(\"/\")[-1]) is not None:\n",
    "                # print(file_path)\n",
    "                try:\n",
    "                    content = pickle.load(open(file_path, 'rb'))\n",
    "                    # Store content with full path as key\n",
    "                    file_contents[file_path] = content\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not read {file_path}: {e}\")\n",
    "    \n",
    "    return file_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b57422d-a837-4228-a4b2-6c71bb96cf18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d775b7de-f7e9-48be-8171-367dff4f2e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6ca4b75-dad1-4de5-abab-cf3d58b8fb79",
   "metadata": {},
   "source": [
    "# functions to collate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe7736cf-c9b2-4227-b971-c50d85bec2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the explained variance objectives for train and validation\n",
    "\n",
    "def extract_explained_variance(data: list[list[PreprocessingSteps]]) -> np.array:\n",
    "\n",
    "    explained_variance = []\n",
    "    \n",
    "    for j in range(len(data)):\n",
    "        \n",
    "        data_run = [data[j][i][-1] for i in range(len(data[j]))]\n",
    "        explained_variance.append(data_run)\n",
    "\n",
    "    return np.array(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e016e4b-c70f-432b-8425-091f5cc85e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94fa952c-8ff2-4746-a443-7412f41eff94",
   "metadata": {},
   "source": [
    "# visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "704bba60-93e4-4064-8240-436ae65bedca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:34: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:34: SyntaxWarning: invalid escape sequence '\\l'\n",
      "/var/folders/d5/3my7z8_d7tb6sgm4462k6n7w0000gn/T/ipykernel_13693/1941458314.py:34: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  plt.ylabel(\"Proportion of explained variance for $\\lambda_1$\", fontsize=14)\n"
     ]
    }
   ],
   "source": [
    "def plot_percentiles(partial_data, full_data, dataset: str):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Colors for different contribution types\n",
    "    colors = {'partial': 'darkred', 'full': 'darkblue'}\n",
    "    linestyles = ['--', '-']\n",
    "    activation_fn = 'relu'\n",
    "    if dataset == 'alternate_stripes':\n",
    "        activation_fn = 'cos'\n",
    "    # Plot both partial and full contributions\n",
    "    for data, contrib_type in [(partial_data, 'partial'), (full_data, 'full')]:\n",
    "        # Compute percentiles along the first axis (runs)\n",
    "\n",
    "        print(f\"FOR contrib type {contrib_type}\")\n",
    "        print(f\"mean {np.mean(data, axis=0)[-1]}\")\n",
    "        print(f\"std {np.std(data, axis=0)[-1]}\")\n",
    "        \n",
    "        p20 = np.percentile(data, 20, axis=0)\n",
    "        p50 = np.percentile(data, 50, axis=0)\n",
    "        p80 = np.percentile(data, 80, axis=0)\n",
    "        # Create x-axis (time steps)\n",
    "        x = np.arange(data.shape[1])\n",
    "        # Plot for each dimension (training/validation)\n",
    "        for dim in range(2):\n",
    "            # Only add label for the first dimension to avoid duplicates in legend\n",
    "            label = f\"h={activation_fn}, objective={contrib_type}\" if dim == 0 else None\n",
    "            plt.plot(x, p50[:, dim], color=colors[contrib_type],\n",
    "                     label=label,\n",
    "                     linestyle=linestyles[dim], linewidth=1.0)\n",
    "            plt.fill_between(x, p20[:, dim], p80[:, dim],\n",
    "                             alpha=0.2, color=colors[contrib_type])\n",
    "    \n",
    "    # Increase font size for axis labels\n",
    "    plt.xlabel('Time steps', fontsize=14)\n",
    "    plt.ylabel(\"Proportion of explained variance for $\\lambda_1$\", fontsize=14)\n",
    "    \n",
    "    # Increase legend size\n",
    "    plt.legend(loc='upper left', fontsize=12)\n",
    "    \n",
    "    # Increase tick label size\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    path_to_save = f\"../results/plots/training_curve/{dataset}.pdf\"\n",
    "    directory = os.path.dirname(path_to_save)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    plt.savefig(path_to_save, dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa6324d3-4b46-4304-91ce-3015da65f457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30005\n",
      "FOR contrib type partial\n",
      "mean [0.32826058 0.31245515]\n",
      "std [0.01455818 0.01731625]\n",
      "FOR contrib type full\n",
      "mean [0.2761214  0.26602167]\n",
      "std [0.01334964 0.01326353]\n"
     ]
    }
   ],
   "source": [
    "# Modified main loop\n",
    "dataset_list = [\"alternate_stripes\",\n",
    "                \"circles\",\n",
    "                \"spheres\", \n",
    "                \"wine\", \n",
    "                \"heart-statlog\", \n",
    "                \"ionosphere\", \n",
    "                \"breast_cancer\", \n",
    "                \"german_credit\"]\n",
    "\n",
    "dataset_list = [\"ionosphere\"]\n",
    "\n",
    "for dataset in dataset_list:\n",
    "    activation = \"relu\"\n",
    "    if dataset == \"alternate_stripes\":\n",
    "        activation = \"cos\"\n",
    "        \n",
    "    data_dictionary = read_all_nested_files(dataset, activation)\n",
    "    \n",
    "    partial_contrib_data = [value for key, value in data_dictionary.items() \n",
    "                            if ('partial_contrib=True' in key)\n",
    "                            and (activation in key)\n",
    "                            and ((\"results_list.p\" in key) or (re.search(r'^[^a-zA-Z]*\\d+\\.p', key.split(\"/\")[-1]) is not None))]\n",
    "    full_contrib_data = [value for key, value in data_dictionary.items() \n",
    "                         if ('partial_contrib=False' in key)\n",
    "                         and (activation in key)\n",
    "                         and ((\"results_list.p\" in key) or (re.search(r'^[^a-zA-Z]*\\d+\\.p', key.split(\"/\")[-1]) is not None))]\n",
    "\n",
    "    # print(len(partial_contrib_data))\n",
    "    \n",
    "    explained_variance_partial = extract_explained_variance(partial_contrib_data)\n",
    "    explained_variance_full = extract_explained_variance(full_contrib_data)\n",
    "    \n",
    "    # Plot both contributions on the same figure\n",
    "    plot_percentiles(explained_variance_partial, explained_variance_full, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ba4a2-0c4d-4a68-9e07-2a3e01c6ef5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61809b86-07da-4cbf-8844-1808dc625f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1ce38-d683-43f1-bde6-62fa5ac13574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50303d3-cc38-44f9-bb16-d61b3765ba2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ce457-2859-4871-8af0-f442c019da5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c51ee9-e366-4962-abbd-f38a3fb0f0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273943fe-06b5-42cc-96ad-cd029525fa27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
